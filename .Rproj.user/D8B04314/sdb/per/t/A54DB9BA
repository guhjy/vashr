{
    "contents" : "#library(SQUAREM)\n#library(gaussquad)\n\n\n# If x is a n-column vector, turn it into n by 1 matrix\n# If x is a matrix, keep it\n#' Title\ntomatrix = function(x){\n  if(is.vector(x)){\n    x = as.matrix(x)\n  }\n  return(x)\n}\n\n# To simplify computation, compute a part of post_pi_vash in advance\n#' Title\ngetA = function(n,k,v,alpha.vec,modalpha.vec,sehat){\n  A = v/2*log(v/2)-lgamma(v/2)+(v/2-1)*outer(rep(1,k),2*log(sehat))+outer(alpha.vec*log(modalpha.vec)-lgamma(alpha.vec)+lgamma(alpha.vec+v/2),rep(1,n))\n  return(A)\n}\n\n#estimate mixture proportions of se's prior by EM algorithm\n#prior gives the parameter of a Dirichlet prior on pi\n#(prior is used to encourage results towards smallest value of sigma when\n#likelihood is flat)\n#' Title \nEMest_se = function(sehat,g,prior,maxiter=5000, v,unimodal, singlecomp){ \n  \n  pi.init = g$pi\n  k = ncomp(g)\n  n = length(sehat)\n  tol = min(0.1/n,1e-4) # set convergence criteria to be more stringent for larger samples\n  \n  if(unimodal=='variance'){\n    c.init = g$beta[1]/(g$alpha[1]+1)\n  }else if(unimodal=='precision'){\n    c.init = g$beta[1]/(g$alpha[1]-1)\n  }\n  c.init = max(c.init,1e-5)\n  \n  EMfit = IGmixEM(sehat, v, c.init, g$alpha, pi.init, prior, unimodal,singlecomp, tol, maxiter)\n  \n  loglik = EMfit$B # actually return log lower bound not log-likelihood! \n  converged = EMfit$converged\n  niter = EMfit$niter\n  loglik.final = EMfit$B[length(EMfit$B)]\n  \n  g$pi=EMfit$pihat \n  g$c=EMfit$chat \n  if(singlecomp==TRUE){\n    g$alpha=EMfit$alphahat\n  }\n  if(unimodal=='variance'){\n    g$beta=g$c*(g$alpha+1)\n  }else if(unimodal=='precision'){\n    g$beta=g$c*(g$alpha-1)\n  }\n  \n  return(list(loglik=loglik.final,converged=converged,g=g,niter=niter))\n}\n\n\n#' Title\nIGmixEM = function(sehat, v, c.init, alpha.vec, pi.init, prior, unimodal,singlecomp, tol, maxiter){\n  q = length(pi.init)\n  n = length(sehat)\n  \n  if(unimodal=='variance'){\n    modalpha.vec=alpha.vec+1\n  }else if(unimodal=='precision'){\n    modalpha.vec=alpha.vec-1\n  }\n  \n  \n  if(singlecomp==FALSE){\n    params.init=c(log(c.init),pi.init)\n    A=getA(n=n,k=q,v,alpha.vec=alpha.vec,modalpha.vec=modalpha.vec,sehat=sehat)\n    \n    res = squarem(par=params.init,fixptfn=fixpoint_se, objfn=penloglik_se, \n                  A=A,n=n,k=q,alpha.vec=alpha.vec,modalpha.vec=modalpha.vec,v=v,sehat=sehat,prior=prior,\n                  control=list(maxiter=maxiter,tol=tol))\n    return(list(chat = exp(res$par[1]), pihat=res$par[2:(length(res$par))], B=-res$value.objfn, \n                niter = res$iter, converged=res$convergence))\n  }else{\n    params.init=c(log(c.init),log(alpha.vec))\n    res=optim(params.init,loglike.se.ac,gr=gradloglike.se.ac,method='L-BFGS-B',\n              lower=c(NA,0), upper=c(NA,log(100)),\n              n=n,k=1,v=v,sehat=sehat,pi=pi,unimodal=unimodal,\n              control=list(maxit=maxiter,pgtol=tol))\n    return(list(chat = exp(res$par[1]), pihat=1, B=-res$value, \n                niter = res$counts[1], converged=res$convergence,\n                alphahat=exp(res$par[2])))\n  }\n  \n}\n\n\n# Estimate the single inv-gamma prior distn params (moments matching)\n# Prior: s^2~IG(a,b)\n# sehat^2~s^2*Gamma(df/2,df/2)\n#' Title\nmomentm = function(sehat,df){\n  n = length(sehat)\n  e = 2*log(sehat)-digamma(df/2)+log(df/2)\n  ehat = mean(e)\n  a = solve_trigamma(mean((e-ehat)^2*n/(n-1)-trigamma(df/2)))\n  b = a*exp(ehat+digamma(df/2)-log(df/2))\n  return(list(a=a,b=b))\n}\n\n# Solve trigamma(y)=x\n#' Title\nsolve_trigamma = function(x){\n  if(x > 1e7){\n    y.new = 1/sqrt(x)\n  }else if (x < 1e-6){\n    y.new = 1/x\n  }else{    \n    y.old = 0.5+1/x\n    delta = trigamma(y.old)*(1-trigamma(y.old)/x)/psigamma(y.old,deriv=2)\n    y.new = y.old+delta\n    while(-delta/y.new <= 1e-8){\n      y.old = y.new\n      delta = trigamma(y.old)*(1-trigamma(y.old)/x)/psigamma(y.old,deriv=2)\n      y.new = y.old+delta\n    }\n  }\n  return(y.new)\n}\n\n# prior of se: se|pi,alpha.vec,c ~ pi*IG(alpha_i,c*(alpha_i-1))\n# Likelihood: sehat^2|se^2 ~ sj*Gamma(v/2,v/2)\n# pi, alpha.vec, c: known\n# Posterior weight of P(se|sehat) (IG mixture distn)\n#' Title \npost_pi_vash = function(A,n,k,v,sehat,alpha.vec,modalpha.vec,c,pi){\n  post.pi.mat=t(pi*exp(A+alpha.vec*log(c)-(alpha.vec+v/2)*log(outer(c*modalpha.vec,v/2*sehat^2,FUN=\"+\"))))\n  return(pimat=post.pi.mat)\n}\n\n#' Title\nfixpoint_se = function(params,A,n,k,alpha.vec,modalpha.vec,v,sehat,prior){\n  logc=params[1]\n  pi=params[2:(length(params))]\n\n  mm = post_pi_vash(A,n,k,v,sehat,alpha.vec,modalpha.vec,exp(logc),pi)\n  m.rowsum = rowSums(mm)\n  classprob = mm/m.rowsum\n  newpi = colSums(classprob)+prior-1\n  newpi = ifelse(newpi<1e-5,1e-5,newpi)\n  newpi = newpi/sum(newpi);\n  est=optim(logc,loglike.se,gr=gradloglike.se,method='BFGS',n=n,k=k,alpha.vec=alpha.vec,modalpha.vec=modalpha.vec,v=v,sehat=sehat,pi=newpi)\n  newc=exp(est$par[1])\n  newlogc=est$par[1]\n  params = c(newlogc,newpi)\n  return(params)\n}\n\n#' Title\npenloglik_se = function(params,A,n,k,alpha.vec,modalpha.vec,v,sehat,prior){\n  c=exp(params[1])\n  pi=params[2:(length(params))]\n  iter=params[length(params)]\n  priordens = sum((prior-1)*log(pi))\n  mm = post_pi_vash(A,n,k,v,sehat,alpha.vec,modalpha.vec,c,pi)\n  m.rowsum = rowSums(mm)\n  loglik = sum(log(m.rowsum))\n  return(-(loglik+priordens))\n}\n\n# Log-likelihood: L(sehat^2|c,pi,alpha.vec)\n#' Title \nloglike.se = function(logc,n,k,alpha.vec,modalpha.vec,v,sehat,pi){  \n  c=exp(logc)\n  pimat = outer(rep(1,n),pi)*exp(v/2*log(v/2)-lgamma(v/2)\n                                 +(v/2-1)*outer(2*log(sehat),rep(1,k))\n                                 +outer(rep(1,n),alpha.vec*log(c*modalpha.vec)-lgamma(alpha.vec)+lgamma(alpha.vec+v/2))\n                                 -outer(rep(1,n),alpha.vec+v/2)*log(outer(v/2*sehat^2,c*modalpha.vec,FUN=\"+\")))\n  logl = sum(log(rowSums(pimat)))\n  return(-logl)\n}\n\n# Gradient of funtion loglike.se (w.r.t logc)\n#' Title\ngradloglike.se = function(logc,n,k,alpha.vec,modalpha.vec,v,sehat,pi){\n  c=exp(logc)\n  \n  pimat = outer(rep(1,n),pi)*exp(v/2*log(v/2)-lgamma(v/2)\n                                 +(v/2-1)*outer(2*log(sehat),rep(1,k))\n                                 +outer(rep(1,n),alpha.vec*log(c*modalpha.vec)-lgamma(alpha.vec)+lgamma(alpha.vec+v/2))\n                                 -outer(rep(1,n),alpha.vec+v/2)*log(outer(v/2*sehat^2,c*modalpha.vec,FUN=\"+\")))\n  classprob = pimat/rowSums(pimat)\n  gradmat = c*classprob*(outer(rep(1,n),alpha.vec/c)\n                         -outer(rep(1,n),(alpha.vec+v/2)*modalpha.vec)/\n                           (outer(v/2*sehat^2,c*modalpha.vec,FUN='+')))\n  grad = sum(-gradmat)\n  return(grad)\n}\n\n# Log-likelihood: L(sehat|c,pi,alpha.vec)\n#' Title\nloglike.se.a = function(logalpha.vec,c,n,k,v,sehat,pi,unimodal){  \n  alpha.vec=exp(logalpha.vec)\n  if(unimodal=='variance'){\n    modalpha.vec=alpha.vec+1\n  }else if(unimodal=='precision'){\n    modalpha.vec=alpha.vec-1\n  }\n  pimat = outer(rep(1,n),pi)*exp(v/2*log(v/2)-lgamma(v/2)\n                                 +(v/2-1)*outer(2*log(sehat),rep(1,k))\n                                 +outer(rep(1,n),alpha.vec*log(c*modalpha.vec)-lgamma(alpha.vec)+lgamma(alpha.vec+v/2))\n                                 -outer(rep(1,n),alpha.vec+v/2)*log(outer(v/2*sehat^2,c*modalpha.vec,FUN=\"+\")))\n  logl = sum(log(rowSums(pimat)))\n  return(-logl)\n}\n\n# Gradient of funtion loglike.se for single component prior (w.r.t logalpha)\n#' Title\ngradloglike.se.a = function(logalpha.vec,c,n,k,v,sehat,pi,unimodal){\n  alpha.vec=exp(logalpha.vec)\n  if(unimodal=='variance'){\n    modalpha.vec=alpha.vec+1\n  }else if(unimodal=='precision'){\n    modalpha.vec=alpha.vec-1\n  }\n  grad=-alpha.vec*sum(log(c)+log(modalpha.vec)+alpha.vec/modalpha.vec-digamma(alpha.vec)+digamma(alpha.vec+v/2)\n                      -c*(alpha.vec+v/2)/(c*modalpha.vec+v/2*sehat^2)-log(c*modalpha.vec+v/2*sehat^2))\n  return(grad)\n}\n\n# Log-likelihood: L(sehat|c,pi,alpha.vec)\n#' Title \nloglike.se.ac = function(params,n,k,v,sehat,pi,unimodal){\n  c=exp(params[1])\n  alpha.vec=exp(params[2:length(params)])\n  if(unimodal=='variance'){\n    modalpha.vec=alpha.vec+1\n  }else if(unimodal=='precision'){\n    modalpha.vec=alpha.vec-1\n  }\n  pimat = outer(rep(1,n),pi)*exp(v/2*log(v/2)-lgamma(v/2)\n                                 +(v/2-1)*outer(2*log(sehat),rep(1,k))\n                                 +outer(rep(1,n),-lgamma(alpha.vec)+lgamma(alpha.vec+v/2))\n                                 +outer(rep(1,n),alpha.vec)*(log(outer(rep(1,n),c*modalpha.vec))-log(outer(v/2*sehat^2,c*modalpha.vec,FUN=\"+\")))\n                                 -(v/2)*log(outer(v/2*sehat^2,c*modalpha.vec,FUN=\"+\")))\n  #classprob=pimat/rowSums(pimat)\n  logl = sum(log(rowSums(pimat)))\n  return(-logl)\n}\n\n# Gradient of funtion loglike.se for single component prior (w.r.t logc and logalpha)\n#' Title\ngradloglike.se.ac=function(params,n,k,v,sehat,pi,unimodal){\n  c=exp(params[1])\n  alpha.vec=exp(params[2:(length(params))])\n  if(unimodal=='variance'){\n    modalpha.vec=alpha.vec+1\n  }else if(unimodal=='precision'){\n    modalpha.vec=alpha.vec-1\n  }\n  pimat = outer(rep(1,n),pi)*exp(v/2*log(v/2)-lgamma(v/2)\n                                 +(v/2-1)*outer(2*log(sehat),rep(1,k))\n                                 +outer(rep(1,n),alpha.vec*log(c*modalpha.vec)-lgamma(alpha.vec)+lgamma(alpha.vec+v/2))\n                                 -outer(rep(1,n),alpha.vec+v/2)*log(outer(v/2*sehat^2,c*modalpha.vec,FUN=\"+\")))\n  \n  classprob = pimat/rowSums(pimat)\n  gradmat.c = c*classprob*(outer(rep(1,n),alpha.vec/c)\n                           -outer(rep(1,n),(alpha.vec+v/2)*modalpha.vec)/\n                             (outer(v/2*sehat^2,c*modalpha.vec,FUN='+')))\n  grad.c = sum(-gradmat.c)\n  \n  grad.a=-alpha.vec*sum(log(c)+log(modalpha.vec)+alpha.vec/modalpha.vec-digamma(alpha.vec)+digamma(alpha.vec+v/2)\n                        -c*(alpha.vec+v/2)/(c*modalpha.vec+v/2*sehat^2)-log(c*modalpha.vec+v/2*sehat^2))\n  return(c(grad.c,grad.a))\n}\n\n#compute posterior shape (alpha1) and rate (beta1)\n#' Title\npost.igmix = function(m,betahat,sebetahat,v){\n  n = length(sebetahat)\n  alpha1 = outer(rep(1,n),m$alpha+v/2)\n  beta1 = outer(m$beta,v/2*sebetahat^2,FUN=\"+\")\n  ismissing = is.na(sebetahat)\n  beta1[,ismissing]=m$beta\n  return(list(alpha=alpha1,beta=t(beta1)))\n}\n\n# Moderated t test\n#' Title\nmod_t_test=function(betahat,se,pi,v){\n  n=length(betahat)\n  k=length(pi)/n\n  pvalue=rep(NA,n)\n  completeobs=(!is.na(betahat) & !is.na(apply(se,1,sum)))\n  temppvalue=pt(outer(betahat[completeobs],rep(1,k))/se[completeobs,],df=v,lower.tail=TRUE)\n  temppvalue=pmin(temppvalue,1-temppvalue)*2\n  pvalue[completeobs]=apply(pi[completeobs,]*temppvalue,1,sum)\n  return(pvalue)\n}\n\n\n#' \n#' @title  Main Variance Adaptive SHrinkage function\n#'\n#' @description Takes vectors of estimates (betahat) and their standard errors (sehat), and applies\n#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.\n#'\n#' @details See readme for more details\n#' \n#' @param sehat, a p vector of standard errors\n#' @param df: appropriate degrees of freedom for (chi-square) distribution of sehat\n#' @param betahat: a p vector of estimates\n#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)\n#' @param singlecomp: bool, indicating whether to use a single inverse-gamma distribution as the prior distribution for the variances\n#' @param unimodal: unimodal constraint for the prior distribution of the variances (\"variance\") or the precisions (\"precision\")\n#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to \"uniform\", or 1,1...,1; also can be \"nullbiased\" 1,1/k-1,...,1/k-1 to put more weight on first component)\n#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the \"true\" g)\n#' @param maxiter: maximum number of iterations of the EM algorithm\n#' \n#'\n#' @return a list with elements fitted.g is fitted mixture\n#' \n#' @export\n#' \n#' @examples \n#' se = rigamma(100,1,1)\n#' sehat = se*rchisq(100,5)/5\n#' se.vash = vash(sehat,5)\n#' plot(sehat,se.vash$PosteriorMean,xlim=c(0,10),ylim=c(0,10))\nvash = function(sehat,df,\n                betahat=NULL,\n                randomstart=FALSE,   \n                singlecomp = FALSE,\n                unimodal = c(\"variance\",\"precision\"),\n                prior=NULL,\n                g=NULL,\n                maxiter = 5000){\n  \n  #method provides a convenient interface to set a particular combinations of parameters for prior an\n  #If method is supplied, use it to set up specific values for these parameters; provide warning if values\n  #are also specified by user\n  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)\n  \n  \n  \n  if(missing(unimodal)){\n    unimodal = match.arg(unimodal) \n  }\n  if(!is.element(unimodal,c(\"variance\",\"precision\"))) stop(\"Error: invalid type of singlecomp\")  \n  \n  completeobs = (!is.na(sehat))\n  n=sum(completeobs)\n  \n  if(n==0){\n    stop(\"Error: all input values are missing\")\n  }  \n  \n  if(!missing(g)){\n    maxiter = 1 # if g is specified, don't iterate the EM\n    prior = rep(1,length(g$pi)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning\n    l = length(g$pi)\n  } else {   \n    mm = momentm(sehat[completeobs],df)\n    mm$a = max(mm$a,1e-5)\n    if(singlecomp==TRUE){\n      alpha = mm$a\n    }else{\n      if(mm$a>1){\n        alpha = 1+((64/mm$a)^(1/6))^seq(-3,6)*(mm$a-1)\n      }else{\n        alpha = mm$a*2^seq(0,13)\n      }\n      #alpha=alpha[alpha<=70] # avoid numerical error\n    }\n    mm$b = max(mm$b, 1e-5)\n    if(unimodal=='precision'){\n      alpha = unique(pmax(alpha,1+1e-5)) # alpha<=1 not allowed\n      beta = mm$b/(mm$a-1)*(alpha-1)\n    }else if(unimodal=='variance'){\n      beta = mm$b/(mm$a+1)*(alpha+1)\n    }\n    \n    l = length(alpha)\n  }\n  \n  \n  if(missing(prior)){\n    prior = rep(1,l)\n  }\n  \n  if(randomstart){\n    pi.se = rgamma(l,1,1)\n  } else {   \n    pi.se=rep(1,l)/l\n  }\n  pi.se=pi.se/sum(pi.se)\n  \n  if (!missing(g)){\n    g=igmix(g$pi,g$alpha,g$beta)\n  }else{\n    g=igmix(pi.se,alpha,beta)\n  }\n  \n  if(length(prior)!=l){\n    stop(\"invalid prior specification\")\n  }\n  \n  pi.fit.se = EMest_se(sehat[completeobs],g,prior,maxiter,df,unimodal, singlecomp)\n  post.se = post.igmix(pi.fit.se$g,rep(numeric(0),n),sehat[completeobs],df)\n  postpi.se = t(matrix(rep(pi.fit.se$g$pi,length(sehat)),ncol=length(sehat)))\n  postpi.se[completeobs,] = t(comppostprob(pi.fit.se$g,rep(numeric(0),n),sehat[completeobs],df))\n  \n  PosteriorMean.se = rep(pi.fit.se$g$c,length=length(sehat))\n  #PosteriorSD.se = rep(0,length=n)\n  \n  \n  #PosteriorSD.se[completeobs] = postsd(pi.fit.se$g,NULL,sehat[completeobs],df) \n  PosteriorShape.se = t(matrix(rep(pi.fit.se$g$alpha,length(sehat)),ncol=length(sehat)))\n  PosteriorShape.se[completeobs,] = post.se$alpha\n  PosteriorRate.se = t(matrix(rep(pi.fit.se$g$beta,length(sehat)),ncol=length(sehat)))\n  PosteriorRate.se[completeobs,] = post.se$beta\n  \n  #PosteriorMean.se[completeobs] = sqrt(postmean(pi.fit.se$g,rep(numeric(0),n),sehat[completeobs],df))\n  PosteriorMean.se = sqrt(apply(postpi.se*PosteriorRate.se/PosteriorShape.se,1,sum))\n  \n  if(length(betahat)==n){\n    pvalue=mod_t_test(betahat,sqrt(PosteriorRate.se/PosteriorShape.se),\n                      postpi.se,PosteriorShape.se*2)\n    qvalue=qvalue(pvalue)$qval\n  }else if(length(betahat)==0){\n    pvalue=NULL\n    qvalue=NULL\n  }else{\n    warning(\"betahat has different length as sehat, cannot compute moderated t-tests\")\n    pvalue=NULL\n    qvalue=NULL\n  }\n  \n  result = list(fitted.g=pi.fit.se$g,\n                PosteriorMean=PosteriorMean.se,\n                #PosteriorSD=PosteriorSD.se,\n                PosteriorShape=PosteriorShape.se,\n                PosteriorRate=PosteriorRate.se,\n                PosteriorPi=postpi.se,\n                pvalue=pvalue,\n                qvalue=qvalue,\n                fit=pi.fit.se,call=match.call(),data=list(sehat=sehat))\n  class(result)= \"vash\"\n  return(result)\n}\n",
    "created" : 1415993728007.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "275660370",
    "id" : "A54DB9BA",
    "lastKnownWriteTime" : 1430686134,
    "path" : "/Volumes/PERSONAL/MS/myvash/package/vash/R/vash.R",
    "project_path" : "R/vash.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}